{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19fb0d5d-e224-4d76-9ab3-a4d8979d3485",
   "metadata": {},
   "source": [
    "# ELT workshop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126a48f2-20d6-4d0f-86de-7bb66989491b",
   "metadata": {},
   "source": [
    "## Build a simple Prefect flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d347224-f58b-4b45-b6db-bb479ce2fbd5",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac4f64e-11cd-437a-be2b-61858f5abc47",
   "metadata": {},
   "source": [
    "Here we're building the simplest possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25af4b08-bf07-47c2-93fa-79dd764bea0a",
   "metadata": {},
   "source": [
    "### Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2e4944a-154e-4460-87d3-37325de549ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import Flow, task\n",
    "\n",
    "@task\n",
    "def echo(text):\n",
    "    return text\n",
    "\n",
    "with Flow(\"Hello, world - local\") as flow:\n",
    "    hello_world = echo(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40b3ff95-605e-4b17-8c53-4f180dbb3a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"73pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 72.99 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 68.99,-40 68.99,4 -4,4\"/>\n",
       "<!-- 140238865515376 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>140238865515376</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"32.5\" cy=\"-18\" rx=\"32.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"32.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">echo</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7f8be7c349d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "702cfb30-1d03-4304-8a9c-7931916b6b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-09 23:08:24+0000] INFO - prefect.FlowRunner | Beginning Flow run for 'My first Prefect flow'\n",
      "[2022-01-09 23:08:24+0000] INFO - prefect.TaskRunner | Task 'echo': Starting task run...\n",
      "[2022-01-09 23:08:25+0000] INFO - prefect.TaskRunner | Task 'echo': Finished task run for task with final state: 'Success'\n",
      "[2022-01-09 23:08:25+0000] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    }
   ],
   "source": [
    "state = flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d928a2b8-09fd-4a1e-9c5c-0ea97a88eb3f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "Check the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bea17713-60ff-413a-85ef-b7cbfd6b1637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Result: 'Hello, world!'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_ref = flow.get_tasks()[0]\n",
    "state.result[task_ref]._result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ad144-a9a2-4fa7-ace5-9ad69721f85e",
   "metadata": {},
   "source": [
    "### Check your understanding\n",
    "- what is a flow?\n",
    "- what is a task?\n",
    "- would using Prefect be a good idea if you only had 5 daily scripts to run?\n",
    "- can you come up with a use case where you could use Prefect (personal or work)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef14a84-db30-4d5f-a460-2c651c57b22a",
   "metadata": {},
   "source": [
    "## Run it from the cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e0402e-d95f-4704-b0d0-491aa0b9036d",
   "metadata": {},
   "source": [
    "Add storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfce75f8-4f23-4ad8-a848-34f1751ae63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect import Flow, task\n",
    "from prefect.storage import GitHub\n",
    "\n",
    "\n",
    "@task\n",
    "def echo(text):\n",
    "    return text\n",
    "\n",
    "\n",
    "STORAGE = GitHub(\n",
    "    repo=\"dyvenia/elt_workshop\",\n",
    "    path=\"elt_workshop/prefect/hello_world_sbx.py\"\n",
    ")\n",
    "\n",
    "\n",
    "with Flow(\"Hello, world - sandbox\", storage=STORAGE) as flow:\n",
    "    hello_world = echo(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17c3c7-009e-40d1-8ef0-5a07b0afaef2",
   "metadata": {},
   "source": [
    "The flow runs just the same (it doens't pull the repo when ran locally):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d96595a8-61b3-4544-bc19-88ac8d97d5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2022-01-09 21:49:50+0000] INFO - prefect.FlowRunner | Beginning Flow run for 'My first Prefect flow'\n",
      "[2022-01-09 21:49:50+0000] INFO - prefect.TaskRunner | Task 'echo': Starting task run...\n",
      "[2022-01-09 21:49:50+0000] INFO - prefect.TaskRunner | Task 'echo': Finished task run for task with final state: 'Success'\n",
      "[2022-01-09 21:49:50+0000] INFO - prefect.FlowRunner | Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    }
   ],
   "source": [
    "state = flow.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ad9acf-da62-4b4a-9619-a84253ea5ae4",
   "metadata": {},
   "source": [
    "Register the flow in Prefect Cloud:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ccccdc-76c1-4767-8d85-90923f59c351",
   "metadata": {},
   "source": [
    "First, create a project where the flow will live:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5996f37-ca68-49f4-ae50-5325dd4b787c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32melt_workshop created\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect create project \"elt_workshop\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68c46b-70c9-4243-999f-055b8dad9afc",
   "metadata": {},
   "source": [
    "Now we can send the flow's metadata to Prefect Cloud:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af2455f0-8d9a-455f-b364-682e3e00f44f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow URL: https://cloud.prefect.io/trymzet/flow/ae7e2d8f-9bcf-4a84-80f4-de0e52fc300a\n",
      " └── ID: d639349a-f31b-4c30-82c0-66c72757b959\n",
      " └── Project: elt_workshop\n",
      " └── Labels: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'d639349a-f31b-4c30-82c0-66c72757b959'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.register(project_name=\"elt_workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18346282-23a2-4291-ace2-03352ba0884e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Use Chrome etc. to open the above link as Prefect UI doesn't look well in Firefox\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77562617-ed1f-4dbc-b7ba-7c7f0256719f",
   "metadata": {},
   "source": [
    "### Check your understanding\n",
    "- why do we need to specify the flow's storage when running it from the cloud?\n",
    "- where are flows triggered by Prefect Cloud running?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba70ce54-4f40-444a-acb2-52790695cce8",
   "metadata": {},
   "source": [
    "## Build a production-grade simple Prefect flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171b96f-7ee7-442a-b34e-7521ea70b5e8",
   "metadata": {},
   "source": [
    "When deploying to production, we want to add several components:\n",
    "\n",
    "1. Specify a reproducible environment in which the flow will be executed  \n",
    "Because reproducibility is very important. \n",
    "\n",
    "By default, Docker Agent uses the `prefecthq/prefect:latest` image.\n",
    "\n",
    "2. Add alerts in case our flow failure  \n",
    "Allows us to take immediate action.\n",
    "\n",
    "3. Add scheduling  \n",
    "We usually want to automate repeatable tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d1dd49-3b9a-41e0-b480-a769c0e5b53a",
   "metadata": {},
   "source": [
    "For point no. 2, we will need to do a bit of set up and config work:\n",
    "> These instructions are intentionally left high-level -- you're welcome to create a PR that changes these vague instructions into recipes for extra points!\n",
    "1. [Set up a Slack workspace](https://slack.com/help/articles/206845317-Create-a-Slack-workspace)\n",
    "2. Follow [this](https://prefect-slack.appspot.com/) link to connect Prefect notifications to a Slack channel.\n",
    "3. Add the webhook url to your Prefect Cloud secrets with the name `SLACK_WEBHOOK_URL`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8980d8e-b40e-4655-8ddd-ac5abe6be51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from prefect.storage import GitHub\n",
    "from prefect.run_configs.docker import DockerRun\n",
    "from prefect.utilities.notifications import slack_notifier\n",
    "from prefect.schedules import Schedule\n",
    "from prefect.schedules.clocks import CronClock\n",
    "from prefect.engine.state import Failed\n",
    "from prefect import Flow, task\n",
    "\n",
    "\n",
    "STORAGE = GitHub(\n",
    "    repo=\"dyvenia/elt_workshop\",\n",
    "    path=\"elt_workshop/prefect/hello_world_dev.py\"\n",
    ")\n",
    "RUN_CONFIG = DockerRun(\n",
    "    image=\"prefecthq/prefect:0.15.11-python3.9\",\n",
    "    env={\"HTTP_PROXY\": \"SOME_IP_ADDR\"},\n",
    "    labels=[\"dev\"],\n",
    ")\n",
    "SLACK_NOTIFIER = slack_notifier(only_states=[Failed])\n",
    "SCHEDULE = Schedule(clocks=[CronClock(\"* */12 * * *\")])\n",
    "\n",
    "@task\n",
    "def echo(text):\n",
    "    return text\n",
    "\n",
    "with Flow(\n",
    "    \"Hello, world - dev\",\n",
    "    storage=STORAGE,\n",
    "    run_config=RUN_CONFIG,\n",
    "    state_handlers=[SLACK_NOTIFIER],\n",
    "    schedule=SCHEDULE,\n",
    ") as flow:\n",
    "    hello_world = echo(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0364f3d-1591-411a-b9e3-511a42dafd25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mdev created\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect create project \"dev\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03a05f9f-91aa-4d42-9a15-5a814336a17a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow URL: https://cloud.prefect.io/trymzet/flow/2992babe-8539-478c-be0e-8a5456f5b70a\n",
      " └── ID: 937a45de-a108-485b-ad55-b96cf4b08bb0\n",
      " └── Project: dev\n",
      " └── Labels: ['dev']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'937a45de-a108-485b-ad55-b96cf4b08bb0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.register(project_name=\"dev\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8617614d-6a00-4cfe-a3d3-6e5ac0399765",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    If you click the link and go to the flow now, you will see an alert next to \"labels\". <br/><br/>\n",
    "    This is because the agent we've been using so far does not have the \"dev\" label. In order to fix this, we need to start another agent with a \"dev\" label. Run `start_prefect_agent_dev.sh` to do that.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ee4c02-c6fe-4d6f-b393-02b764ab098f",
   "metadata": {},
   "source": [
    "### Understanding\n",
    "- why is pinning the version of the Docker image important? What could happen if we don't pin it and simply use `latest`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9fdc9-1771-45db-bce4-ef5d985dec07",
   "metadata": {},
   "source": [
    "## Build a Prefect flow to run an Airbyte sync"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466a70cb-5d48-46aa-9173-f51d5d09d233",
   "metadata": {},
   "source": [
    "## `AirbyteConnectionTask2` \n",
    "This is needed until the PR to Prefect is accepted. I cleaned up the messy logs from the original Prefect task and added some improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c677e91f-4d7b-4fb5-b459-df44f926bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "import uuid\n",
    "\n",
    "import requests\n",
    "from requests import RequestException\n",
    "\n",
    "from prefect import Task\n",
    "from prefect.engine.signals import FAIL\n",
    "from prefect.utilities.tasks import defaults_from_attrs\n",
    "import logging\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class ConnectionNotFoundException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class AirbyteServerNotHealthyException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class JobNotFoundException(Exception):\n",
    "    pass\n",
    "\n",
    "\n",
    "class AirbyteConnectionTask2(Task):\n",
    "    \"\"\"\n",
    "    Task for triggering Airbyte Connections, where \"A connection is\n",
    "    a configuration for syncing data between a source and a destination.\"\n",
    "    For more information refer to the\n",
    "    [Airbyte docs](https://docs.airbyte.io/understanding-airbyte/connections)\n",
    "    This task assumes that the Airbyte Open-Source, since \"For\n",
    "    Airbyte Open-Source you don't need the API Token for\n",
    "    Authentication! All endpoints are possible to access using the\n",
    "    API without it.\"\n",
    "    For more information refer to the [Airbyte docs](https://docs.airbyte.io/api-documentation)\n",
    "    Args:\n",
    "        - airbyte_server_host (str, optional): Hostname of Airbyte server where connection is configured.\n",
    "            Defaults to localhost.\n",
    "        - airbyte_server_port (str, optional): Port that the Airbyte server is listening on.\n",
    "            Defaults to 8000.\n",
    "        - airbyte_api_version (str, optional): Version of Airbyte API to use to trigger connection sync.\n",
    "            Defaults to v1.\n",
    "        - connection_id (str, optional): Default connection id to\n",
    "            use for sync jobs, if none is specified to `run`.\n",
    "        - **kwargs (Any, optional): additional kwargs to pass to the\n",
    "            base Task constructor\n",
    "    \"\"\"\n",
    "\n",
    "    # Connection statuses\n",
    "    CONNECTION_STATUS_ACTIVE = \"active\"\n",
    "    CONNECTION_STATUS_INACTIVE = \"inactive\"\n",
    "    CONNECTION_STATUS_DEPRECATED = \"deprecated\"\n",
    "\n",
    "    # Job statuses\n",
    "    # pending┃running┃incomplete┃failed┃succeeded┃cancelled\n",
    "    JOB_STATUS_SUCCEEDED = \"succeeded\"\n",
    "    JOB_STATUS_FAILED = \"failed\"\n",
    "    JOB_STATUS_PENDING = \"pending\"\n",
    "    \n",
    "    # Attempt statuses\n",
    "    ATTEMPT_STATUS_SUCCEEDED = \"succeeded\"\n",
    "    ATTEMPT_STATUS_FAILED = \"failed\"\n",
    "    ATTEMPT_STATUS_RUNNING = \"running\"\n",
    "    \n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        airbyte_server_host: str = \"localhost\",\n",
    "        airbyte_server_port: int = 8000,\n",
    "        airbyte_api_version: str = \"v1\",\n",
    "        connection_id: str = None,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.airbyte_server_host = airbyte_server_host\n",
    "        self.airbyte_server_port = airbyte_server_port\n",
    "        self.airbyte_api_version = airbyte_api_version\n",
    "        self.connection_id = connection_id\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _check_health_status(self, session: requests.Session) -> bool:\n",
    "        get_connection_url = self.airbyte_base_url + \"/health/\"\n",
    "        try:\n",
    "            response = session.get(get_connection_url)\n",
    "            health_status = response.json()[\"db\"]\n",
    "            if not health_status:\n",
    "                raise AirbyteServerNotHealthyException(\n",
    "                    f\"Airbyte Server health status: {health_status}\"\n",
    "                )\n",
    "            return True\n",
    "        except RequestException as e:\n",
    "            raise AirbyteServerNotHealthyException(e)\n",
    "            \n",
    "    def _get_connection_source_and_dest(self, session: requests.Session) -> Tuple[str, str]:\n",
    "        \"\"\"Get source and destination names for the connection ID.\n",
    "        Note `web_backend` in the URL\"\"\"\n",
    "        get_connection_url = self.airbyte_base_url + \"/web_backend/connections/get/\"\n",
    "        try:\n",
    "            response = session.post(\n",
    "                get_connection_url, json={\"connectionId\": self.connection_id}\n",
    "            )\n",
    "            source = response.json()[\"source\"][\"name\"]\n",
    "            destination = response.json()[\"destination\"][\"name\"]\n",
    "            return source, destination\n",
    "        except RequestException as e:\n",
    "            raise AirbyteServerNotHealthyException(e)\n",
    "\n",
    "    def _get_connection_status(self, session: requests.Session) -> str:\n",
    "        get_connection_url = self.airbyte_base_url + \"/connections/get/\"\n",
    "        \n",
    "        # TODO - Missing authentication because Airbyte servers currently do not support authentication\n",
    "        try:\n",
    "            response = session.post(\n",
    "                get_connection_url, json={\"connectionId\": self.connection_id}\n",
    "            )\n",
    "\n",
    "            # check whether a schedule exists ...\n",
    "            schedule = response.json()[\"schedule\"]\n",
    "            if schedule:\n",
    "                self.logger.warning(\"Found existing Connection schedule, removing ...\")\n",
    "\n",
    "                # mandatory fields for Connection update ...\n",
    "                sync_catalog = response.json()[\"syncCatalog\"]\n",
    "                connection_status = response.json()[\"status\"]\n",
    "\n",
    "                update_connection_url = self.airbyte_base_url + \"/connections\" \"/update/\"\n",
    "                response2 = session.post(\n",
    "                    update_connection_url,\n",
    "                    json={\n",
    "                        \"connectionId\": self.connection_id,\n",
    "                        \"syncCatalog\": sync_catalog,\n",
    "                        \"schedule\": None,\n",
    "                        \"status\": connection_status,\n",
    "                    },\n",
    "                )\n",
    "\n",
    "                if response2.status_code == 200:\n",
    "                    self.logger.info(\"Schedule removed.\")\n",
    "                else:\n",
    "                    self.logger.warning(\"Schedule not removed.\")\n",
    "                    self.logger.warning(response2.json())\n",
    "\n",
    "            connection_status = response.json()[\"status\"]\n",
    "            return connection_status\n",
    "        except RequestException as e:\n",
    "            raise AirbyteServerNotHealthyException(e)\n",
    "\n",
    "    def _trigger_manual_sync_connection(self, session: requests.Session) -> Tuple[int, int]:\n",
    "        \"\"\"\n",
    "        Trigger a manual sync of the Connection, see:\n",
    "        https://airbyte-public-api-docs.s3.us-east-2.amazonaws.com/rapidoc\n",
    "        -api-docs.html#post-/v1/connections/sync\n",
    "        Args:\n",
    "            session: requests session with which to make call to Airbyte server\n",
    "        Returns: job_id, job_created_at - timestamp of sync job creation\n",
    "        \"\"\"\n",
    "        get_connection_url = self.airbyte_base_url + \"/connections/sync/\"\n",
    "\n",
    "        # TODO - missing authentication ...\n",
    "        try:\n",
    "            response = session.post(\n",
    "                get_connection_url, json={\"connectionId\": self.connection_id}\n",
    "            )\n",
    "            if response.status_code == 200:\n",
    "                job_id = response.json()[\"job\"][\"id\"]\n",
    "                job_created_at = response.json()[\"job\"][\"createdAt\"]\n",
    "                return job_id, job_created_at\n",
    "            elif response.status_code == 404:\n",
    "                raise ConnectionNotFoundException(\n",
    "                    f\"Connection {connection_id} not found, please double \"\n",
    "                    f\"check the connection_id ...\"\n",
    "                )\n",
    "        except RequestException as e:\n",
    "            raise AirbyteServerNotHealthyException(e)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _escape_ansi(line: str) -> str:\n",
    "        ansi_escape = re.compile(r'(?:\\x1B[@-_]|[\\x80-\\x9F])[0-?]*[ -/]*[@-~]')\n",
    "        return ansi_escape.sub('', line)\n",
    "    \n",
    "    def _get_stripped_logger(self):\n",
    "        \"\"\"Helper method for _log_airbyte_logs()\"\"\"\n",
    "        root_logger = logging.getLogger(\"prefect\")\n",
    "        \n",
    "        handler = [h for h in root_logger.handlers if type(h) == logging.StreamHandler][0]\n",
    "        prev_formatter = handler.formatter\n",
    "        formatter = logging.Formatter(fmt='%(message)s')\n",
    "        handler.setFormatter(formatter)\n",
    "        \n",
    "        stripped_logger = root_logger.getChild(self.name)\n",
    "        \n",
    "        return stripped_logger, prev_formatter\n",
    "    \n",
    "    def _recover_prefect_logger(self, prev_formatter: logging.Formatter) -> None:\n",
    "        \"\"\"Helper method for _log_airbyte_logs()\"\"\"\n",
    "        root_logger = logging.getLogger(\"prefect\")\n",
    "        \n",
    "        handler = [h for h in root_logger.handlers if type(h) == logging.StreamHandler][0]\n",
    "        handler.setFormatter(prev_formatter)\n",
    "    \n",
    "    def _log_airbyte_logs(self, logs: List[str]) -> None:\n",
    "        \"\"\"Take a list of Airbyte logs and log them with the Prefect logger\"\"\"\n",
    "        logger, prev_formatter = self._get_stripped_logger()\n",
    "        for log in logs:\n",
    "            log_escaped = self._escape_ansi(log)\n",
    "            logger.debug(log_escaped)\n",
    "        self._recover_prefect_logger(prev_formatter)\n",
    "\n",
    "    def _get_job_status(\n",
    "        self, session: requests.Session, job_id: int\n",
    "    ) -> Tuple[str, int, int]:\n",
    "        get_connection_url = self.airbyte_base_url + \"/jobs/get/\"\n",
    "        try:\n",
    "            response = session.post(get_connection_url, json={\"id\": job_id})\n",
    "            if response.status_code == 200:\n",
    "                job_status = response.json()[\"job\"][\"status\"]\n",
    "                \n",
    "                if job_status == self.JOB_STATUS_SUCCEEDED:\n",
    "                    attempts = response.json()[\"attempts\"]\n",
    "                    successful_attempt = [\n",
    "                        a for a in attempts if a[\"attempt\"][\"status\"] == self.ATTEMPT_STATUS_SUCCEEDED\n",
    "                    ][0]\n",
    "\n",
    "                    self._log_airbyte_logs(successful_attempt[\"logs\"][\"logLines\"])\n",
    "                                     \n",
    "                job_created_at = response.json()[\"job\"][\"createdAt\"]\n",
    "                job_updated_at = response.json()[\"job\"][\"updatedAt\"]\n",
    "                return job_status, job_created_at, job_updated_at\n",
    "            elif response.status_code == 404:\n",
    "                self.logger.error(f\"Job {job_id} not found...\")\n",
    "                raise JobNotFoundException(f\"Job {job_id} not found...\")\n",
    "        except RequestException as e:\n",
    "            raise AirbyteServerNotHealthyException(e)\n",
    "\n",
    "    @defaults_from_attrs(\n",
    "        \"airbyte_server_host\",\n",
    "        \"airbyte_server_port\",\n",
    "        \"airbyte_api_version\",\n",
    "        \"connection_id\",\n",
    "    )\n",
    "    def run(\n",
    "        self,\n",
    "        airbyte_server_host: str = None,\n",
    "        airbyte_server_port: int = None,\n",
    "        airbyte_api_version: str = None,\n",
    "        connection_id: str = None,\n",
    "        display_airbyte_logs: bool = None,\n",
    "        poll_interval_s: int = 15,\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Task run method for triggering an Airbyte Connection.\n",
    "        *It is assumed that the user will have previously configured\n",
    "        a Source & Destination into a Connection.*\n",
    "        e.g. MySql -> CSV\n",
    "        An invocation of `run` will attempt to start a sync job for\n",
    "        the specified `connection_id` representing the Connection in\n",
    "        Airbyte.\n",
    "        `run` will poll Airbyte Server for the Connection status and\n",
    "        will only complete when the sync has completed or\n",
    "        when it receives an error status code from an API call.\n",
    "        Args:\n",
    "            - airbyte_server_host (str, optional): Hostname of Airbyte server where connection is\n",
    "                configured. Will overwrite the value provided at init if provided.\n",
    "            - airbyte_server_port (str, optional): Port that the Airbyte server is listening on.\n",
    "                Will overwrite the value provided at init if provided.\n",
    "            - airbyte_api_version (str, optional): Version of Airbyte API to use to trigger connection\n",
    "                sync. Will overwrite the value provided at init if provided.\n",
    "            - connection_id (str, optional): if provided,\n",
    "                will overwrite the value provided at init.\n",
    "            - poll_interval_s (int, optional): this task polls the\n",
    "                Airbyte API for status, if provided this value will\n",
    "                override the default polling time of 15 seconds.\n",
    "        Returns:\n",
    "            - dict: connection_id (str) and succeeded_at (timestamp str)\n",
    "        \"\"\"\n",
    "        \n",
    "        if not connection_id:\n",
    "            raise ValueError(\"`connection_id` *must* be provided.\")\n",
    "\n",
    "        try:\n",
    "            uuid.UUID(connection_id)\n",
    "        except (TypeError, ValueError):\n",
    "            raise ValueError(\n",
    "                \"Parameter `connection_id` *must* be a valid UUID \\\n",
    "                i.e. 32 hex characters, including hyphens.\"\n",
    "            )\n",
    "            \n",
    "        self.connection_id = connection_id\n",
    "\n",
    "        # see https://airbyte-public-api-docs.s3.us-east-2.amazonaws.com\n",
    "        # /rapidoc-api-docs.html#overview\n",
    "        self.airbyte_base_url = (\n",
    "            f\"http://{airbyte_server_host}:\"\n",
    "            f\"{airbyte_server_port}/api/{airbyte_api_version}\"\n",
    "        )\n",
    "\n",
    "        session = requests.Session()\n",
    "        self._check_health_status(session)\n",
    "\n",
    "        connection_status = self._get_connection_status(session)\n",
    "        if connection_status == self.CONNECTION_STATUS_ACTIVE:\n",
    "            # Trigger manual sync on the Connection ...\n",
    "            source, destination = self._get_connection_source_and_dest(session)\n",
    "            self.logger.info(f\"Triggering Airbyte sync '{source}' -> '{destination}'...\")\n",
    "                \n",
    "            job_id, job_created_at = self._trigger_manual_sync_connection(session)\n",
    "\n",
    "            job_status = self.JOB_STATUS_PENDING\n",
    "\n",
    "            while job_status not in [self.JOB_STATUS_FAILED, self.JOB_STATUS_SUCCEEDED]:\n",
    "                job_status, job_created_at, job_updated_at = self._get_job_status(\n",
    "                    session, job_id\n",
    "                )\n",
    "\n",
    "                if job_status == self.JOB_STATUS_SUCCEEDED:\n",
    "                    self.logger.info(f\"Job {job_id} succeeded.\")\n",
    "                elif job_status == self.JOB_STATUS_FAILED:\n",
    "                    self.logger.error(f\"Job {job_id} failed.\")\n",
    "                else:\n",
    "                    # wait for next poll interval\n",
    "                    sleep(poll_interval_s)\n",
    "\n",
    "            return {\n",
    "                \"connection_id\": connection_id,\n",
    "                \"status\": connection_status,\n",
    "                \"job_status\": job_status,\n",
    "                \"job_created_at\": job_created_at,\n",
    "                \"job_updated_at\": job_updated_at,\n",
    "            }\n",
    "        elif connection_status == self.CONNECTION_STATUS_INACTIVE:\n",
    "            self.logger.error(\n",
    "                f\"Please enable the Connection {connection_id} in Airbyte Server.\"\n",
    "            )\n",
    "            raise FAIL(\n",
    "                f\"Please enable the Connection {connection_id} in Airbyte Server.\"\n",
    "            )\n",
    "        elif connection_status == self.CONNECTION_STATUS_DEPRECATED:\n",
    "            self.logger.error(f\"Connection {connection_id} is deprecated.\")\n",
    "            raise FAIL(f\"Connection {connection_id} is deprecated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3b4092b-ab04-4bcb-9324-650f90020c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning Flow run for 'Determine common contributors flow'\n",
      "Task 'AIRBYTE_POKE_CONNECTION_ID': Starting task run...\n",
      "Task 'AIRBYTE_POKE_CONNECTION_ID': Finished task run for task with final state: 'Success'\n",
      "Task 'AirbyteConnectionTask2': Starting task run...\n",
      "Triggering Airbyte sync 'workshop_poke' -> 'workshop_local_json'...\n",
      "2021-12-24 15:19:14 INFO i.a.s.a.w.WorkerRun(call):49 - Executing worker wrapper. Airbyte version: 0.34.2-alpha\n",
      "2021-12-24 15:19:14 INFO i.a.w.t.TemporalAttemptExecution(get):118 - Docker volume job log path: /tmp/workspace/54/0/logs.log\n",
      "2021-12-24 15:19:14 INFO i.a.w.t.TemporalAttemptExecution(get):123 - Executing worker wrapper. Airbyte version: 0.34.2-alpha\n",
      "2021-12-24 15:19:14 WARN i.a.d.Databases(createPostgresDatabaseWithRetryTimeout):65 - Waiting for database to become available...\n",
      "2021-12-24 15:19:14 INFO i.a.d.i.j.JobsDatabaseInstance(lambda$static$2):25 - Testing if jobs database is ready...\n",
      "2021-12-24 15:19:14 INFO i.a.d.Databases(createPostgresDatabaseWithRetryTimeout):90 - Database available!\n",
      "2021-12-24 15:19:14 INFO i.a.d.Databases(createPostgresDatabaseWithRetry):48 - Database available!\n",
      "2021-12-24 15:19:14 INFO i.a.w.DefaultReplicationWorker(run):99 - start sync worker. job id: 54 attempt id: 0\n",
      "2021-12-24 15:19:14 INFO i.a.w.DefaultReplicationWorker(run):108 - configured sync modes: {null.pokemon=full_refresh - overwrite}\n",
      "2021-12-24 15:19:14 INFO i.a.w.p.a.DefaultAirbyteDestination(start):68 - Running destination...\n",
      "2021-12-24 15:19:14 INFO i.a.c.i.LineGobbler(voidCall):82 - Checking if airbyte/destination-local-json:0.2.8 exists...\n",
      "2021-12-24 15:19:14 INFO i.a.c.i.LineGobbler(voidCall):82 - airbyte/destination-local-json:0.2.8 was found locally.\n",
      "2021-12-24 15:19:14 INFO i.a.w.p.DockerProcessFactory(create):171 - Preparing command: docker run --rm --init -i -v airbyte_workspace:/data -v /tmp/airbyte_local:/local -w /data/54/0 --network host --log-driver none airbyte/destination-local-json:0.2.8 write --config destination_config.json --catalog destination_catalog.json\n",
      "2021-12-24 15:19:14 INFO i.a.c.i.LineGobbler(voidCall):82 - Checking if airbyte/source-pokeapi:0.1.4 exists...\n",
      "2021-12-24 15:19:15 INFO i.a.c.i.LineGobbler(voidCall):82 - airbyte/source-pokeapi:0.1.4 was found locally.\n",
      "2021-12-24 15:19:15 INFO i.a.w.p.DockerProcessFactory(create):171 - Preparing command: docker run --rm --init -i -v airbyte_workspace:/data -v /tmp/airbyte_local:/local -w /data/54/0 --network host --log-driver none airbyte/source-pokeapi:0.1.4 read --config source_config.json --catalog source_catalog.json\n",
      "2021-12-24 15:19:15 INFO i.a.w.DefaultReplicationWorker(run):136 - Waiting for source thread to join.\n",
      "2021-12-24 15:19:15 INFO i.a.w.DefaultReplicationWorker(lambda$getReplicationRunnable$2):207 - Replication thread started.\n",
      "2021-12-24 15:19:15 INFO i.a.w.DefaultReplicationWorker(lambda$getDestinationOutputRunnable$3):243 - Destination output thread started.\n",
      "2021-12-24 15:19:15 source > Starting syncing SourcePokeapi\n",
      "2021-12-24 15:19:15 source > Syncing stream: pokemon \n",
      "2021-12-24 15:19:15 source > Read 1 records from pokemon stream\n",
      "2021-12-24 15:19:15 source > Finished syncing SourcePokeapi\n",
      "2021-12-24 15:19:15 source > SourcePokeapi runtimes:\n",
      "\n",
      "2021-12-24 15:19:15 source > Finished syncing SourcePokeapi\n",
      "2021-12-24 15:19:15 INFO i.a.w.DefaultReplicationWorker(run):138 - Source thread complete.\n",
      "2021-12-24 15:19:15 INFO i.a.w.DefaultReplicationWorker(run):139 - Waiting for destination thread to join.\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.b.IntegrationRunner(run):96 - {} - Running integration: io.airbyte.integrations.destination.local_json.LocalJsonDestination\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.b.IntegrationCliParser(parseOptions):135 - {} - integration args: {catalog=destination_catalog.json, write=null, config=destination_config.json}\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.b.IntegrationRunner(run):100 - {} - Command: WRITE\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.b.IntegrationRunner(run):101 - {} - Integration config: IntegrationConfig{command=WRITE, configPath='destination_config.json', catalogPath='destination_catalog.json', statePath='null'}\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 WARN c.n.s.JsonMetaSchema(newValidator):338 - {} - Unknown keyword examples - you should define your own Meta Schema. If the keyword is irrelevant for validation, just use a NonValidationKeyword\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.d.l.LocalJsonDestination$JsonConsumer(<init>):151 - {} - initializing consumer.\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.b.FailureTrackingAirbyteMessageConsumer(close):80 - {} - Airbyte message consumer: succeeded.\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.d.l.LocalJsonDestination$JsonConsumer(close):192 - {} - finalizing consumer.\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.d.l.LocalJsonDestination$JsonConsumer(close):208 - {} - File output: /local/workshop/poke/_airbyte_raw_pokemon.jsonl\n",
      "2021-12-24 15:19:16 destination > 2021-12-24 15:19:16 INFO i.a.i.b.IntegrationRunner(run):153 - {} - Completed integration: io.airbyte.integrations.destination.local_json.LocalJsonDestination\n",
      "2021-12-24 15:19:16 INFO i.a.w.DefaultReplicationWorker(run):141 - Destination thread complete.\n",
      "2021-12-24 15:19:16 INFO i.a.w.DefaultReplicationWorker(run):169 - sync summary: io.airbyte.config.ReplicationAttemptSummary@162d2fdb[status=completed,recordsSynced=1,bytesSynced=266987,startTime=1640359154950,endTime=1640359156597]\n",
      "2021-12-24 15:19:16 INFO i.a.w.DefaultReplicationWorker(run):178 - Source did not output any state messages\n",
      "2021-12-24 15:19:16 WARN i.a.w.DefaultReplicationWorker(run):189 - State capture: No state retained.\n",
      "2021-12-24 15:19:16 INFO i.a.w.t.TemporalAttemptExecution(get):144 - Stopping cancellation check scheduling...\n",
      "2021-12-24 15:19:16 INFO i.a.w.t.s.ReplicationActivityImpl(replicate):144 - sync summary: io.airbyte.config.StandardSyncOutput@7b85d954[standardSyncSummary=io.airbyte.config.StandardSyncSummary@430e3ec3[status=completed,recordsSynced=1,bytesSynced=266987,startTime=1640359154950,endTime=1640359156597],state=<null>,outputCatalog=io.airbyte.protocol.models.ConfiguredAirbyteCatalog@3644092f[streams=[io.airbyte.protocol.models.ConfiguredAirbyteStream@2a51650f[stream=io.airbyte.protocol.models.AirbyteStream@7fcb4668[name=pokemon,jsonSchema={\"type\":\"object\",\"$schema\":\"http://json-schema.org/draft-07/schema#\",\"properties\":{\"id\":{\"type\":[\"null\",\"integer\"]},\"name\":{\"type\":[\"null\",\"string\"]},\"forms\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}}},\"moves\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"move\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}},\"version_group_details\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"version_group\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}},\"level_learned_at\":{\"type\":[\"null\",\"integer\"]},\"move_learn_method\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}}}}}}}},\"order\":{\"type\":[\"null\",\"integer\"]},\"stats\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"stat\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}},\"effort\":{\"type\":[\"null\",\"integer\"]},\"base_stat\":{\"type\":[\"null\",\"integer\"]}}}},\"types\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"slot\":{\"type\":[\"null\",\"integer\"]},\"type\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}}}}},\"height\":{\"type\":[\"null\",\"integer\"]},\"weight\":{\"type\":[\"null\",\"integer\"]},\"species\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}},\"sprites\":{\"type\":[\"null\",\"object\"],\"properties\":{\"back_shiny\":{\"type\":[\"null\",\"string\"]},\"back_female\":{\"type\":[\"null\",\"string\"]},\"front_shiny\":{\"type\":[\"null\",\"string\"]},\"back_default\":{\"type\":[\"null\",\"string\"]},\"front_female\":{\"type\":[\"null\",\"string\"]},\"front_default\":{\"type\":[\"null\",\"string\"]},\"back_shiny_female\":{\"type\":[\"null\",\"string\"]},\"front_shiny_female\":{\"type\":[\"null\",\"string\"]}}},\"abilities\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"slot\":{\"type\":[\"null\",\"integer\"]},\"ability\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}},\"is_hidden\":{\"type\":[\"null\",\"boolean\"]}}}},\"held_items\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"item\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}},\"version_details\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"rarity\":{\"type\":[\"null\",\"integer\"]},\"version\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}}}}}}}},\"is_default \":{\"type\":[\"null\",\"boolean\"]},\"game_indices\":{\"type\":[\"null\",\"array\"],\"items\":{\"type\":[\"null\",\"object\"],\"properties\":{\"version\":{\"type\":[\"null\",\"object\"],\"properties\":{\"url\":{\"type\":[\"null\",\"string\"]},\"name\":{\"type\":[\"null\",\"string\"]}}},\"game_index\":{\"type\":[\"null\",\"integer\"]}}}},\"base_experience\":{\"type\":[\"null\",\"integer\"]},\"location_area_encounters\":{\"type\":[\"null\",\"string\"]}}},supportedSyncModes=[full_refresh],sourceDefinedCursor=<null>,defaultCursorField=[],sourceDefinedPrimaryKey=[],namespace=<null>,additionalProperties={}],syncMode=full_refresh,cursorField=[],destinationSyncMode=overwrite,primaryKey=[],additionalProperties={}]],additionalProperties={}]]\n",
      "Job 54 succeeded.\n",
      "Task 'AirbyteConnectionTask2': Finished task run for task with final state: 'Success'\n",
      "Flow run SUCCESS: all reference tasks succeeded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Success: \"All reference tasks succeeded.\">"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from prefect import Flow, Parameter, task\n",
    "from prefect.run_configs.docker import DockerRun\n",
    "from prefect.storage.local import Local\n",
    "from prefect.tasks.airbyte.airbyte import AirbyteConnectionTask\n",
    "from prefect.tasks.dbt.dbt import DbtShellTask\n",
    "\n",
    "RUN_CONFIG = DockerRun(\n",
    "    image=\"viadot:latest\",\n",
    "    env={\"SOME_VAR\": \"value\"},\n",
    "    labels=[\"dev\"],\n",
    ")\n",
    "\n",
    "sync_airbyte_connection = AirbyteConnectionTask2(\n",
    "    max_retries=3,\n",
    "    retry_delay=timedelta(seconds=10),\n",
    "    airbyte_server_host=\"webapp\",\n",
    "    airbyte_server_port=80,\n",
    "    airbyte_api_version=\"v1\"\n",
    ")\n",
    "\n",
    "\n",
    "with Flow(\"Sync pokeAPI to Metabase - local\", storage=Local(), run_config=RUN_CONFIG) as flow:\n",
    "    # Airbyte connection strings\n",
    "    airbyte_poke_connection_id = Parameter(\"AIRBYTE_POKE_CONNECTION_ID\")\n",
    "\n",
    "    # Sync\n",
    "    sync_poke = sync_airbyte_connection(connection_id=airbyte_poke_connection_id)\n",
    "\n",
    "    \n",
    "# flow.visualize()\n",
    "flow.run(AIRBYTE_POKE_CONNECTION_ID=\"4a6d8e08-936a-4820-bfb6-3c676866adc4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39f3f7b-2e98-4c5b-85bb-662337a5a362",
   "metadata": {},
   "source": [
    "Run below in a terminal before and after the sync to see if the file was actually reloaded:  \n",
    "`cat /tmp/airbyte_local/workshop/poke/_airbyte_raw_pokemon.jsonl | python -m json.tool | grep _airbyte_emitted_at`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e00a355-ee49-4501-898c-dc5b92338cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d409170e-f057-4dfa-9e3f-582c0107cb7a",
   "metadata": {},
   "source": [
    "## Build the full Prefect flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9f6cd8-176d-43c3-b4c0-22b64b4849be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "from prefect import Flow, Parameter, task\n",
    "from prefect.run_configs.docker import DockerRun\n",
    "from prefect.storage.local import Local\n",
    "from prefect.tasks.airbyte.airbyte import AirbyteConnectionTask\n",
    "from prefect.tasks.dbt.dbt import DbtShellTask\n",
    "\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
